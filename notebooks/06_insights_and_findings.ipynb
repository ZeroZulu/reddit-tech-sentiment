{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 06 ‚Äî Insights & Business Findings\n",
    "\n",
    "**Objective:** Synthesize all analyses into actionable business insights and key takeaways for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 294,704 posts\n",
      "Subreddits: 6\n",
      "Date range: 2008-06-23 to 2022-05-08\n",
      "Transformer sample: 9,996 posts\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load final analyzed data\n",
    "df = pd.read_parquet('../data/processed/posts_final.parquet')\n",
    "print(f\"Dataset: {len(df):,} posts\")\n",
    "print(f\"Subreddits: {df['subreddit'].nunique()}\")\n",
    "print(f\"Date range: {pd.to_datetime(df['created_utc']).min().date()} to {pd.to_datetime(df['created_utc']).max().date()}\")\n",
    "\n",
    "# Load transformer comparison sample if available\n",
    "try:\n",
    "    df_sample = pd.read_parquet('../data/processed/sentiment_sample_10k.parquet')\n",
    "    has_transformer = True\n",
    "    print(f\"Transformer sample: {len(df_sample):,} posts\")\n",
    "except FileNotFoundError:\n",
    "    has_transformer = False\n",
    "    print(\"Transformer sample not found ‚Äî VADER-only metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KEY METRICS (computed from data)\n",
      "======================================================================\n",
      "\n",
      "üìä Overall Sentiment:\n",
      "   Avg VADER compound: +0.3038\n",
      "   Positive: 53.9% | Neutral: 35.9% | Negative: 10.2%\n",
      "\n",
      "üèÜ Subreddit Ranking (by avg sentiment):\n",
      "   MachineLearning           +0.2352\n",
      "   artificial                +0.2711\n",
      "   analytics                 +0.3230\n",
      "   computerscience           +0.3282\n",
      "   datascience               +0.3953\n",
      "   dataengineering           +0.4586\n",
      "\n",
      "üî• Most Active: r/MachineLearning (120,765 posts)\n",
      "\n",
      "üìà Engagement Correlations:\n",
      "   Sentiment ‚Üî Score:    r=+0.0109 (p=3.01e-09)\n",
      "   Sentiment ‚Üî Comments: r=+0.1765 (p=0.00e+00)\n",
      "\n",
      "üìå Top Topics:\n",
      "   Help / Would                   40,549 posts (sentiment: +0.482)\n",
      "   Computer / Science / Year      38,285 posts (sentiment: +0.484)\n",
      "   Learning / Machine / Intelligence 31,162 posts (sentiment: +0.165)\n",
      "   Work / Time / People           29,909 posts (sentiment: +0.367)\n",
      "   Function / Algorithm / Problem 25,534 posts (sentiment: +0.211)\n",
      "\n",
      "ü§ñ VADER ‚Üî DistilBERT Agreement: 49.8%\n",
      "   (Low agreement expected ‚Äî SST-2 model trained on movie reviews, not tech text)\n",
      "\n",
      "‚è∞ Peak Activity: Wednesdays at 18:00 UTC\n"
     ]
    }
   ],
   "source": [
    "# Compute all metrics from data ‚Äî nothing hardcoded\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY METRICS (computed from data)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall sentiment\n",
    "avg_sent = df['vader_compound'].mean()\n",
    "pct_pos = (df['vader_label'] == 'positive').mean()\n",
    "pct_neg = (df['vader_label'] == 'negative').mean()\n",
    "pct_neu = (df['vader_label'] == 'neutral').mean()\n",
    "print(f\"\\nüìä Overall Sentiment:\")\n",
    "print(f\"   Avg VADER compound: {avg_sent:+.4f}\")\n",
    "print(f\"   Positive: {pct_pos:.1%} | Neutral: {pct_neu:.1%} | Negative: {pct_neg:.1%}\")\n",
    "\n",
    "# Most positive / negative subreddits\n",
    "sub_sent = df.groupby('subreddit')['vader_compound'].mean().sort_values()\n",
    "print(f\"\\nüèÜ Subreddit Ranking (by avg sentiment):\")\n",
    "for sub, score in sub_sent.items():\n",
    "    print(f\"   {sub:<25} {score:+.4f}\")\n",
    "\n",
    "# Most active subreddit\n",
    "most_active = df['subreddit'].value_counts().idxmax()\n",
    "most_active_count = df['subreddit'].value_counts().max()\n",
    "print(f\"\\nüî• Most Active: r/{most_active} ({most_active_count:,} posts)\")\n",
    "\n",
    "# Engagement-sentiment correlation\n",
    "corr_score = stats.pearsonr(df['vader_compound'], np.log1p(df['score']))\n",
    "corr_comments = stats.pearsonr(df['vader_compound'], np.log1p(df['num_comments']))\n",
    "print(f\"\\nüìà Engagement Correlations:\")\n",
    "print(f\"   Sentiment ‚Üî Score:    r={corr_score[0]:+.4f} (p={corr_score[1]:.2e})\")\n",
    "print(f\"   Sentiment ‚Üî Comments: r={corr_comments[0]:+.4f} (p={corr_comments[1]:.2e})\")\n",
    "\n",
    "# Topic distribution\n",
    "if 'topic_name' in df.columns:\n",
    "    print(f\"\\nüìå Top Topics:\")\n",
    "    for topic, count in df['topic_name'].value_counts().head(5).items():\n",
    "        topic_sent = df[df['topic_name'] == topic]['vader_compound'].mean()\n",
    "        print(f\"   {topic:<30} {count:>6,} posts (sentiment: {topic_sent:+.3f})\")\n",
    "\n",
    "# Transformer comparison (if available)\n",
    "if has_transformer:\n",
    "    # Use same binary mapping as NB03 for consistency\n",
    "    vader_binary = df_sample['vader_label'].apply(\n",
    "        lambda x: 'positive' if x == 'positive' else 'negative'\n",
    "    )\n",
    "    agreement = (vader_binary == df_sample['transformer_label']).mean()\n",
    "    print(f\"\\nü§ñ VADER ‚Üî DistilBERT Agreement: {agreement:.1%}\")\n",
    "    print(f\"   (Low agreement expected ‚Äî SST-2 model trained on movie reviews, not tech text)\")\n",
    "\n",
    "# Posting patterns\n",
    "df['hour'] = pd.to_datetime(df['created_utc']).dt.hour\n",
    "df['dow'] = pd.to_datetime(df['created_utc']).dt.day_name()\n",
    "peak_hour = df['hour'].mode()[0]\n",
    "peak_day = df['dow'].mode()[0]\n",
    "print(f\"\\n‚è∞ Peak Activity: {peak_day}s at {peak_hour}:00 UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## Methodology Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "METHODOLOGY COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Sentiment Analysis: VADER vs DistilBERT\n",
      "--------------------------------------------------\n",
      "Metric                    VADER                DistilBERT          \n",
      "Scale                     Full dataset         10K sample          \n",
      "Output classes            3 (pos/neu/neg)      2 (pos/neg)         \n",
      "Training domain           Rule-based           SST-2 (movies)      \n",
      "Handles sarcasm           Limited              Better (in-domain)  \n",
      "Requires GPU              No                   Optional            \n",
      "Interpretability          High                 Medium              \n",
      "Agreement rate            49.8%\n",
      "\n",
      "Note: DistilBERT's SST-2 checkpoint was not fine-tuned for tech forum text.\n",
      "Low agreement reflects domain mismatch, not a methodology failure.\n",
      "Recommendation: Fine-tune on ~1K labeled tech posts for production use.\n",
      "\n",
      "\n",
      "Topic Modeling: LDA\n",
      "--------------------------------------------------\n",
      "  Topics discovered: 15\n",
      "  Avg assignment confidence: 0.483\n",
      "  Coherence evaluated across k=5,10,15,20 ‚Äî best model selected by c_v\n",
      "  Note: BERTopic not run (requires additional compute). Would likely\n",
      "  produce more coherent topics on short Reddit text.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"METHODOLOGY COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nSentiment Analysis: VADER vs DistilBERT\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Metric':<25} {'VADER':<20} {'DistilBERT':<20}\")\n",
    "print(f\"{'Scale':<25} {'Full dataset':<20} {'10K sample':<20}\")\n",
    "print(f\"{'Output classes':<25} {'3 (pos/neu/neg)':<20} {'2 (pos/neg)':<20}\")\n",
    "print(f\"{'Training domain':<25} {'Rule-based':<20} {'SST-2 (movies)':<20}\")\n",
    "print(f\"{'Handles sarcasm':<25} {'Limited':<20} {'Better (in-domain)':<20}\")\n",
    "print(f\"{'Requires GPU':<25} {'No':<20} {'Optional':<20}\")\n",
    "print(f\"{'Interpretability':<25} {'High':<20} {'Medium':<20}\")\n",
    "\n",
    "if has_transformer and 'agreement' in dir():\n",
    "    print(f\"{'Agreement rate':<25} {agreement:.1%}\")\n",
    "\n",
    "print(\"\\nNote: DistilBERT's SST-2 checkpoint was not fine-tuned for tech forum text.\")\n",
    "print(\"Low agreement reflects domain mismatch, not a methodology failure.\")\n",
    "print(\"Recommendation: Fine-tune on ~1K labeled tech posts for production use.\")\n",
    "\n",
    "print(\"\\n\\nTopic Modeling: LDA\")\n",
    "print(\"-\" * 50)\n",
    "if 'topic_name' in df.columns:\n",
    "    n_topics = df['topic_name'].nunique()\n",
    "    avg_conf = df['lda_topic_prob'].mean() if 'lda_topic_prob' in df.columns else None\n",
    "    print(f\"  Topics discovered: {n_topics}\")\n",
    "    if avg_conf is not None:\n",
    "        print(f\"  Avg assignment confidence: {float(avg_conf):.3f}\")\n",
    "    else:\n",
    "        print(f\"  Avg assignment confidence: N/A\")\n",
    "    print(f\"  Coherence evaluated across k=5,10,15,20 ‚Äî best model selected by c_v\")\n",
    "    print(f\"  Note: BERTopic not run (requires additional compute). Would likely\")\n",
    "    print(f\"  produce more coherent topics on short Reddit text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Limitations & Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMITATIONS:\n",
      "  ‚Ä¢ Data sourced from Kaggle datasets (2008‚Äì2022), not live Reddit API\n",
      "  ‚Ä¢ VADER struggles with tech-specific sarcasm (e.g., 'great, another AI tool')\n",
      "  ‚Ä¢ DistilBERT SST-2 trained on movie reviews ‚Äî domain mismatch with tech text\n",
      "    causes systematic negative bias (neutral tech posts ‚Üí negative classification)\n",
      "  ‚Ä¢ DistilBERT only produces binary (pos/neg) ‚Äî no neutral class\n",
      "  ‚Ä¢ LDA topic quality depends on preprocessing; short posts are challenging\n",
      "  ‚Ä¢ No comment-level analysis (post titles and body text only)\n",
      "  ‚Ä¢ DistilBERT ran on 10K sample, not full dataset\n",
      "\n",
      "FUTURE WORK:\n",
      "  ‚Ä¢ Fine-tune DistilBERT on tech-specific labeled data (~1K labeled posts)\n",
      "  ‚Ä¢ Add real-time Reddit streaming for live sentiment monitoring\n",
      "  ‚Ä¢ Run BERTopic for comparison with LDA\n",
      "  ‚Ä¢ Correlate sentiment with actual stock price movements (lag analysis)\n",
      "  ‚Ä¢ Deploy to Streamlit Cloud for public access\n",
      "  ‚Ä¢ Expand to HackerNews for cross-platform comparison\n"
     ]
    }
   ],
   "source": [
    "print(\"LIMITATIONS:\")\n",
    "print(\"  ‚Ä¢ Data sourced from Kaggle datasets (2008‚Äì2022), not live Reddit API\")\n",
    "print(\"  ‚Ä¢ VADER struggles with tech-specific sarcasm (e.g., 'great, another AI tool')\")\n",
    "print(\"  ‚Ä¢ DistilBERT SST-2 trained on movie reviews ‚Äî domain mismatch with tech text\")\n",
    "print(\"    causes systematic negative bias (neutral tech posts ‚Üí negative classification)\")\n",
    "print(\"  ‚Ä¢ DistilBERT only produces binary (pos/neg) ‚Äî no neutral class\")\n",
    "print(\"  ‚Ä¢ LDA topic quality depends on preprocessing; short posts are challenging\")\n",
    "print(\"  ‚Ä¢ No comment-level analysis (post titles and body text only)\")\n",
    "print(\"  ‚Ä¢ DistilBERT ran on 10K sample, not full dataset\")\n",
    "print()\n",
    "print(\"FUTURE WORK:\")\n",
    "print(\"  ‚Ä¢ Fine-tune DistilBERT on tech-specific labeled data (~1K labeled posts)\")\n",
    "print(\"  ‚Ä¢ Add real-time Reddit streaming for live sentiment monitoring\")\n",
    "print(\"  ‚Ä¢ Run BERTopic for comparison with LDA\")\n",
    "print(\"  ‚Ä¢ Correlate sentiment with actual stock price movements (lag analysis)\")\n",
    "print(\"  ‚Ä¢ Deploy to Streamlit Cloud for public access\")\n",
    "print(\"  ‚Ä¢ Expand to HackerNews for cross-platform comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Skills Demonstrated\n",
    "\n",
    "- **NLP:** Text preprocessing, sentiment analysis (VADER + DistilBERT transformer), LDA topic modeling\n",
    "- **Data Engineering:** Kaggle dataset integration, SQLite storage, parquet ETL pipeline\n",
    "- **Machine Learning:** Model comparison, coherence-based hyperparameter selection, ensemble scoring\n",
    "- **Statistical Analysis:** Correlation analysis, z-score anomaly detection, time series trends\n",
    "- **Visualization:** Interactive dashboards (Streamlit + Plotly), publication-quality charts\n",
    "- **Software Engineering:** Modular architecture, unit testing (pytest), YAML configuration, documentation\n",
    "\n",
    "---\n",
    "\n",
    "*Project by Shril Patel ‚Äî [GitHub](https://github.com/shrilpatel) | [LinkedIn](https://linkedin.com/in/shrilpatel)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
